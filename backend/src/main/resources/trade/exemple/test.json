Points clés (bugs/correctifs)
Fuite de données dans la normalisation: la normalisation est recalculée sur train+test, et la cible close n’utilise pas la même méthode/scope que les features. Unifier via des “scalers” appris sur train uniquement et réutilisés partout (CV, eval, prédiction).
Mauvais usage LSTM à l’inférence: seule la dernière étape de la fenêtre est passée au réseau. Il faut passer la séquence complète [batch, features, time] et récupérer la dernière sortie.
evaluateModel refait un split 80/20 et reshape les sorties pour matcher à tout prix. En CV, il faut évaluer sur le fold test passé et ne pas reshaper à l’aveugle.
Arrays.asList(double[]) ne permute pas (tableaux primitifs). Bug dans l’importance par permutation.
SHAP: basePred est dénormalisé alors que pertPred est en espace normalisé. Comparaison incohérente d’unités.
Découpage TimeSeriesSplit: la condition if (testStart - windowSize < windowSize) est incorrecte; vérifier qu’il y a au moins windowSize+1 points côté train.
Logging: les formats {: .3f} ne fonctionnent pas avec SLF4J; utiliser {} ou String.format.
Mémoire et threads DL4J: nettoyer workspaces/GC après chaque config/fold; éventuellement utiliser ParallelWrapper ou EarlyStopping.
Améliorations de conception
Utiliser un état de normalisation persistant (train only) et l’enregistrer avec le modèle (ou en base) pour une prédiction cohérente.
Bascule vers un schéma many-to-one: entrée = séquence complète, sortie = valeur suivante du close.
Early stopping natif DL4J (EarlyStoppingTrainer) avec ScoreCalculator sur validation, plutôt que patience artisanale.
Seed global pour reproductibilité: Nd4j.getRandom().setSeed(...) et seed dans la config DL4J.
Seuil swing plus robuste: ATR% ou percentile des retours absolus, pas max(1% prix, abs diff moyen).
Réduire la verbosité logs et uniformiser les shapes avant MSE sans reshape forcé.