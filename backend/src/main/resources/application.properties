# Cl? API OpenAI
openai.api.key=${OPENAI_API_KEY}
openai.api.url=https://api.openai.com/v1/chat/completions
openai.api.model=gpt-5-mini

trade.type=swing

# indice
signal.mix.active=false
signal.lstm.active=false
signal.single.active=true

# API finnhub twelvedata, statistic, finalcial data
finnhub.api.key=${CLE_FINNHUB}
finnhub.api.url=https://finnhub.io/api/v1/
finnhub.api.history.limit=50

# API twelvedata, indicateur technique
twelvedata.api.key=${CLE_TWELVEDATA}
twelvedata.api.url=https://api.twelvedata.com/
twelvedata.api.history.limit=50

# API eodhd, news withsentiment
eodhd.api.key=${CLE_EODHD}
eodhd.api.url=https://eodhistoricaldata.com/api/
eodhd.api.news.limit=300

# API alpaca markets, trading et market data
alpaca.markets.api.paper.url=https://paper-api.alpaca.markets/v2
alpaca.markets.api.live.url=https://api.alpaca.markets
alpaca.markets.api.market.url=https://data.alpaca.markets
alpaca.markets.api.order.limit=300
alpaca.markets.api.news.defaut.limit=5

alphavantage.api.key=${CLE_ALPHA_VANTAGE}
alphavantage.api.url=https://www.alphavantage.co/query?function=
marketaux.api.key=${CLE_MARKETAUX}
marketaux.api.url=https://api.marketaux.com

# log
logging.file.name=app.log
logging.level.root=INFO
logging.level.org.springframework=DEBUG


# db
spring.datasource.url=jdbc:mysql://localhost:3306/trade_ai?useSSL=false&serverTimezone=UTC
#spring.datasource.url=jdbc:mysql://192.168.1.73:3306/trade_ai?allowPublicKeyRetrieval=true&useSSL=false&serverTimezone=UTC
spring.datasource.username=${LOGIN_DB}
#spring.datasource.username=root1
spring.datasource.password=${PASSWORD_DB}
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver

spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL8Dialect

lstm.tuning.enableTwoPhase=true

# Seuils tuning deux phases (gains requis pour accepter phase 2)
# Gain relatif minimal (ex: 0.05 = +5%)
lstm.tuning.twoPhase.minRelativeGain=0.05
# Gain absolu minimal sur le businessScore (ex: +0.02)
lstm.tuning.twoPhase.minAbsoluteGain=0.02

# ---------------------------------------------------------------------------
# GPU / LSTM TUNING ? Paramètres d?optimisation occupation GPU
# ---------------------------------------------------------------------------
# Parallélisme symboles/configs (0 = auto). L?auto limite selon CPU & backend CUDA
lstm.tuning.maxThreads=0

# Concurrence interne GPU contrôlée dynamiquement (sémaphore)
# (MAJ) Augmentation plafond max à 4 et abaissement seuil scaleUp pour occupation plus rapide

gpu.concurrency.min=2
gpu.concurrency.max=4
gpuTargetBatchSize=128
# On augmente si VRAM < 60% désormais (ancien 95%)
gpu.concurrency.scaleUpThresholdPct=60.0
# Seuil de réduction conservé (adapter à 90.0 si besoin de libération plus proactive)
gpu.concurrency.scaleDownThresholdPct=85.0

# Désactivation du "stagger" pour démarrer les trainings immédiatement (maximiser pics GPU)
lstm.tuning.gpu.enableStagger=false
# Activation de l?auto-scale batch (double progressivement jusqu?à la cible)
lstm.tuning.gpu.autoBatchScale=true
# (MAJ) Batch cible aligné sur nouveau batchSize=512
lstm.tuning.gpu.targetBatchSize=128
# Ajuster le learning rate proportionnellement à l?augmentation du batch (conserve stabilité)
lstm.tuning.gpu.scaleLearningRateOnBatch=true

# --- Variante plus prudente (décommenter si OOM ou instabilité) ---
# gpu.concurrency.min=1
# gpu.concurrency.max=4
# gpu.concurrency.scaleUpThresholdPct=80.0
# gpu.concurrency.scaleDownThresholdPct=92.0
# lstm.tuning.gpu.targetBatchSize=128
# lstm.tuning.gpu.enableStagger=true
# lstm.tuning.gpu.scaleLearningRateOnBatch=true
# ---------------------------------------------------------------------------
